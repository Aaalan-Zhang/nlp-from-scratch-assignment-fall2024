{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/alan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "100%|██████████| 161/161 [00:14<00:00, 11.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files processed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Make sure NLTK's sentence and word tokenizers are downloaded\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Function to read the content of a text file\n",
    "def read_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "# Function to split text into sentence-based chunks with word count limit and overlap\n",
    "def shard_text_by_sentences(text, max_words_per_chunk=1024, overlap_sentences=10):\n",
    "    sentences = sent_tokenize(text)  # Split text into sentences\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_word_count = 0\n",
    "\n",
    "    for i in range(len(sentences)):\n",
    "        sentence = sentences[i]\n",
    "        sentence_word_count = len(word_tokenize(sentence))\n",
    "\n",
    "        # If adding this sentence exceeds the max words limit, finalize the current chunk\n",
    "        if current_word_count + sentence_word_count > max_words_per_chunk:\n",
    "            chunks.append(current_chunk)\n",
    "            current_chunk = sentences[max(0, i - overlap_sentences):i + overlap_sentences]\n",
    "            current_word_count = sum(len(word_tokenize(s)) for s in current_chunk)\n",
    "        else:\n",
    "            current_chunk.append(sentence)\n",
    "            current_word_count += sentence_word_count\n",
    "\n",
    "    # Add the last chunk if it exists\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Function to save each chunk into a separate text file\n",
    "def save_shard(chunk, output_dir, file_name, shard_index):\n",
    "    output_file = os.path.join(output_dir, f\"{file_name}-{shard_index}.txt\")\n",
    "    \n",
    "    # Join sentences in the chunk and save to file\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        file.write(' '.join(chunk))\n",
    "\n",
    "# Function to process all text files in a directory\n",
    "def process_directory(input_dir, output_dir, max_words_per_chunk=1024, overlap_sentences=10):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    for file_name in tqdm(os.listdir(input_dir)):\n",
    "        if file_name.endswith(\".txt\"):\n",
    "            file_path = os.path.join(input_dir, file_name)\n",
    "            file_base_name = os.path.splitext(file_name)[0]\n",
    "\n",
    "            # Read file content and shard it\n",
    "            text = read_file(file_path)\n",
    "            shards = shard_text_by_sentences(text, max_words_per_chunk, overlap_sentences)\n",
    "\n",
    "            # Save each shard\n",
    "            for index, shard in enumerate(shards):\n",
    "                save_shard(shard, output_dir, file_base_name, index)\n",
    "\n",
    "    print(\"All files processed.\")\n",
    "\n",
    "# Define input and output directories\n",
    "input_directory = '../../data/crawled/crawled_text_data'  # Directory containing text files\n",
    "output_directory = '../../data/crawled/crawled_text_data_test'  # Directory to save the shards\n",
    "\n",
    "# Process the directory and shard files\n",
    "process_directory(input_directory, output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/alan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "100%|██████████| 161/161 [00:02<00:00, 61.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files processed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Make sure NLTK's sentence and word tokenizers are downloaded\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Function to read the content of a text file\n",
    "def read_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "# Function to split text into word-based chunks with overlap\n",
    "def shard_text_by_words(text, max_words_per_chunk=1024, overlap_words=200):\n",
    "    words = word_tokenize(text)  # Tokenize text into words\n",
    "    chunks = []\n",
    "    step_size = max_words_per_chunk - overlap_words // 2  # Step size for moving the window\n",
    "\n",
    "    for i in range(0, len(words), step_size):\n",
    "        start = max(0, i - overlap_words // 2)  # Ensure we include previous overlap\n",
    "        end = min(len(words), i + max_words_per_chunk + overlap_words // 2)  # Ensure next overlap\n",
    "        chunk = words[start:end]\n",
    "        chunks.append(chunk)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Function to save each chunk into a separate text file\n",
    "def save_shard(chunk, output_dir, file_name, shard_index):\n",
    "    output_file = os.path.join(output_dir, f\"{file_name}-{shard_index}.txt\")\n",
    "    \n",
    "    # Join words in the chunk and save to file\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        file.write(' '.join(chunk))\n",
    "\n",
    "# Function to process all text files in a directory\n",
    "def process_directory(input_dir, output_dir, max_words_per_chunk=1024, overlap_words=200):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    for file_name in tqdm(os.listdir(input_dir)):\n",
    "        if file_name.endswith(\".txt\"):\n",
    "            file_path = os.path.join(input_dir, file_name)\n",
    "            file_base_name = os.path.splitext(file_name)[0]\n",
    "\n",
    "            # Read file content and shard it\n",
    "            text = read_file(file_path)\n",
    "            shards = shard_text_by_words(text, max_words_per_chunk, overlap_words)\n",
    "\n",
    "            # Save each shard\n",
    "            for index, shard in enumerate(shards):\n",
    "                save_shard(shard, output_dir, file_base_name, index)\n",
    "\n",
    "    print(\"All files processed.\")\n",
    "\n",
    "# Define input and output directories\n",
    "input_directory = '../../data/crawled/crawled_text_data'  # Directory containing text files\n",
    "output_directory = '../../data/crawled/crawled_text_data_test'  # Directory to save the shards\n",
    "\n",
    "# Process the directory and shard files\n",
    "process_directory(input_directory, output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/alan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "100%|██████████| 161/161 [00:03<00:00, 40.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files processed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Make sure NLTK's sentence and word tokenizers are downloaded\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Function to read the content of a text file\n",
    "def read_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "# Function to split text into sentence-based chunks with word-based overlap\n",
    "def shard_text_by_sentences(text, max_words_per_chunk=1024, overlap_words=200):\n",
    "    sentences = sent_tokenize(text)  # Tokenize text into sentences\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_word_count = 0\n",
    "\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        sentence_words = word_tokenize(sentence)\n",
    "        sentence_word_count = len(sentence_words)\n",
    "\n",
    "        # Check if adding this sentence will exceed the word limit for the chunk\n",
    "        if current_word_count + sentence_word_count > max_words_per_chunk:\n",
    "            # Finalize the current chunk\n",
    "            chunks.append(current_chunk)\n",
    "\n",
    "            # Reset current_chunk with overlap from the previous chunk and new sentence\n",
    "            overlap = current_chunk[-overlap_words:] if len(current_chunk) > overlap_words else current_chunk\n",
    "            current_chunk = overlap + sentence_words\n",
    "            current_word_count = len(current_chunk)\n",
    "\n",
    "        else:\n",
    "            current_chunk += sentence_words\n",
    "            current_word_count += sentence_word_count\n",
    "\n",
    "    # Add the last chunk if it's not empty\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Function to save each chunk into a separate text file\n",
    "def save_shard(chunk, output_dir, file_name, shard_index):\n",
    "    output_file = os.path.join(output_dir, f\"{file_name}-{shard_index}.txt\")\n",
    "    \n",
    "    # Join words in the chunk and save to file\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        file.write(' '.join(chunk))\n",
    "\n",
    "# Function to process all text files in a directory\n",
    "def process_directory(input_dir, output_dir, max_words_per_chunk=1024, overlap_words=200):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    for file_name in tqdm(os.listdir(input_dir)):\n",
    "        if file_name.endswith(\".txt\"):\n",
    "            file_path = os.path.join(input_dir, file_name)\n",
    "            file_base_name = os.path.splitext(file_name)[0]\n",
    "\n",
    "            # Read file content and shard it\n",
    "            text = read_file(file_path)\n",
    "            shards = shard_text_by_sentences(text, max_words_per_chunk, overlap_words)\n",
    "\n",
    "            # Save each shard\n",
    "            for index, shard in enumerate(shards):\n",
    "                save_shard(shard, output_dir, file_base_name, index)\n",
    "\n",
    "    print(\"All files processed.\")\n",
    "\n",
    "# Define input and output directories\n",
    "input_directory = '../../data/crawled/crawled_text_data'  # Directory containing text files\n",
    "output_directory = '../../data/crawled/crawled_text_data_50_sentence'  # Directory to save the shards\n",
    "\n",
    "# Process the directory and shard files\n",
    "process_directory(input_directory, output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/alan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "  0%|          | 0/688 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 688/688 [00:03<00:00, 223.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files processed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Make sure NLTK's word tokenizer is downloaded\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Function to read the content of a text file\n",
    "def read_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "# Function to split a large chunk into smaller chunks if it exceeds 1024 words\n",
    "def split_chunk_by_words(text, max_words_per_chunk=1024):\n",
    "    words = word_tokenize(text)  # Tokenize the chunk into words\n",
    "    chunks = []\n",
    "    \n",
    "    for i in range(0, len(words), max_words_per_chunk):\n",
    "        chunk = words[i:i + max_words_per_chunk]\n",
    "        chunks.append(chunk)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Function to save the new split chunks into separate text files\n",
    "def save_shard(chunk, output_dir, file_name, shard_index):\n",
    "    output_file = os.path.join(output_dir, f\"{file_name}-{shard_index}.txt\")\n",
    "    \n",
    "    # Join words in the chunk and save to file\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        file.write(' '.join(chunk))\n",
    "\n",
    "# Function to process the directory of chunks and further split chunks larger than 1024 words\n",
    "def process_chunk_directory(input_dir, output_dir, max_words_per_chunk=1024):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    for file_name in tqdm(os.listdir(input_dir)):\n",
    "        if file_name.endswith(\".txt\"):\n",
    "            file_path = os.path.join(input_dir, file_name)\n",
    "            file_base_name = os.path.splitext(file_name)[0]\n",
    "\n",
    "            # Read the chunk file content\n",
    "            text = read_file(file_path)\n",
    "            \n",
    "            # Check the word count of the chunk and split if necessary\n",
    "            chunks = split_chunk_by_words(text, max_words_per_chunk)\n",
    "\n",
    "            # Save the new smaller chunks\n",
    "            for index, chunk in enumerate(chunks):\n",
    "                save_shard(chunk, output_dir, file_base_name, index)\n",
    "\n",
    "    print(\"All files processed.\")\n",
    "\n",
    "# Define input and output directories\n",
    "input_directory = '../../data/crawled/crawled_text_data_sentence_50_5'  # Directory containing text files\n",
    "output_directory = '../../data/crawled/crawled_text_data_max_1024'  # Directory to save the shards\n",
    "\n",
    "# Process the directory and split chunks if needed\n",
    "process_chunk_directory(input_directory, output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_29.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_15.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_114.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_100.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_128.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_129.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_101.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_115.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_14.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_28.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_16.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_103.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_117.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_116.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_102.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_17.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_13.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_106.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_112.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_.DS_Store\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_113.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_107.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_12.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_10.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_38.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_139.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_111.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_105.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_104.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_110.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_138.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_39.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_11.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_76.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_62.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_89.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_88.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_63.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_77.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_49.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_61.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_75.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_148.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_149.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_74.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_60.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_48.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_64.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_70.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_58.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_59.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_71.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_65.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_73.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_67.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_9.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_98.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_99.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_8.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_66.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_72.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_57.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_5.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_43.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_142.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_94.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_80.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_81.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_95.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_143.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_42.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_56.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_4.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_68.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_40.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_6.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_54.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_141.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_83.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_97.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_96.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_82.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_140.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_7.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_55.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_41.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_69.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_45.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_51.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_3.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_79.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_86.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_92.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_144.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_150.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_145.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_93.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_87.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_78.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_50.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_2.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_44.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_0.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_52.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_46.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_91.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_85.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_147.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_146.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_84.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_90.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_47.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_1.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_53.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_34.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_20.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_135.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_121.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_109.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_108.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_120.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_134.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_21.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_35.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_23.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_37.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_122.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_136.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_137.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_123.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_36.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_22.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_26.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_32.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_127.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_133.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_132.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_126.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_33.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_27.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_31.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_25.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_19.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_118.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_130.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_124.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_125.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_131.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_119.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_18.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_24.txt\n",
      "Copied and replaced: ../../data/crawled/crawled_all/parentlink_30.txt\n",
      "All files copied successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Define the source and destination directories\n",
    "source_dir = '/Users/alan/crawled_text_data'  # Change to your source directory\n",
    "destination_dir = '../../data/crawled/crawled_all'  # Change to your destination directory\n",
    "prefix = 'parentlink_'  # Prefix to add to filenames\n",
    "\n",
    "# Ensure the destination directory exists\n",
    "if not os.path.exists(destination_dir):\n",
    "    os.makedirs(destination_dir)\n",
    "\n",
    "# Iterate through all files in the source directory\n",
    "for filename in os.listdir(source_dir):\n",
    "    # Get the full file path\n",
    "    source_file = os.path.join(source_dir, filename)\n",
    "\n",
    "    # Check if it's a file (not a directory)\n",
    "    if os.path.isfile(source_file):\n",
    "        # Create the destination file path with the prefix\n",
    "        destination_file = os.path.join(destination_dir, prefix + filename)\n",
    "\n",
    "        # Copy the file and replace if it exists\n",
    "        shutil.copy2(source_file, destination_file)\n",
    "        print(f\"Copied and replaced: {destination_file}\")\n",
    "\n",
    "print(\"All files copied successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data/crawled/crawled_text_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
