{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/alan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "100%|██████████| 161/161 [00:14<00:00, 11.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files processed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Make sure NLTK's sentence and word tokenizers are downloaded\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Function to read the content of a text file\n",
    "def read_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "# Function to split text into sentence-based chunks with word count limit and overlap\n",
    "def shard_text_by_sentences(text, max_words_per_chunk=1024, overlap_sentences=10):\n",
    "    sentences = sent_tokenize(text)  # Split text into sentences\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_word_count = 0\n",
    "\n",
    "    for i in range(len(sentences)):\n",
    "        sentence = sentences[i]\n",
    "        sentence_word_count = len(word_tokenize(sentence))\n",
    "\n",
    "        # If adding this sentence exceeds the max words limit, finalize the current chunk\n",
    "        if current_word_count + sentence_word_count > max_words_per_chunk:\n",
    "            chunks.append(current_chunk)\n",
    "            current_chunk = sentences[max(0, i - overlap_sentences):i + overlap_sentences]\n",
    "            current_word_count = sum(len(word_tokenize(s)) for s in current_chunk)\n",
    "        else:\n",
    "            current_chunk.append(sentence)\n",
    "            current_word_count += sentence_word_count\n",
    "\n",
    "    # Add the last chunk if it exists\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Function to save each chunk into a separate text file\n",
    "def save_shard(chunk, output_dir, file_name, shard_index):\n",
    "    output_file = os.path.join(output_dir, f\"{file_name}-{shard_index}.txt\")\n",
    "    \n",
    "    # Join sentences in the chunk and save to file\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        file.write(' '.join(chunk))\n",
    "\n",
    "# Function to process all text files in a directory\n",
    "def process_directory(input_dir, output_dir, max_words_per_chunk=1024, overlap_sentences=10):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    for file_name in tqdm(os.listdir(input_dir)):\n",
    "        if file_name.endswith(\".txt\"):\n",
    "            file_path = os.path.join(input_dir, file_name)\n",
    "            file_base_name = os.path.splitext(file_name)[0]\n",
    "\n",
    "            # Read file content and shard it\n",
    "            text = read_file(file_path)\n",
    "            shards = shard_text_by_sentences(text, max_words_per_chunk, overlap_sentences)\n",
    "\n",
    "            # Save each shard\n",
    "            for index, shard in enumerate(shards):\n",
    "                save_shard(shard, output_dir, file_base_name, index)\n",
    "\n",
    "    print(\"All files processed.\")\n",
    "\n",
    "# Define input and output directories\n",
    "input_directory = '../../data/crawled/crawled_text_data'  # Directory containing text files\n",
    "output_directory = '../../data/crawled/crawled_text_data_test'  # Directory to save the shards\n",
    "\n",
    "# Process the directory and shard files\n",
    "process_directory(input_directory, output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/alan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "100%|██████████| 161/161 [00:02<00:00, 61.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files processed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Make sure NLTK's sentence and word tokenizers are downloaded\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Function to read the content of a text file\n",
    "def read_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "# Function to split text into word-based chunks with overlap\n",
    "def shard_text_by_words(text, max_words_per_chunk=1024, overlap_words=200):\n",
    "    words = word_tokenize(text)  # Tokenize text into words\n",
    "    chunks = []\n",
    "    step_size = max_words_per_chunk - overlap_words // 2  # Step size for moving the window\n",
    "\n",
    "    for i in range(0, len(words), step_size):\n",
    "        start = max(0, i - overlap_words // 2)  # Ensure we include previous overlap\n",
    "        end = min(len(words), i + max_words_per_chunk + overlap_words // 2)  # Ensure next overlap\n",
    "        chunk = words[start:end]\n",
    "        chunks.append(chunk)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Function to save each chunk into a separate text file\n",
    "def save_shard(chunk, output_dir, file_name, shard_index):\n",
    "    output_file = os.path.join(output_dir, f\"{file_name}-{shard_index}.txt\")\n",
    "    \n",
    "    # Join words in the chunk and save to file\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        file.write(' '.join(chunk))\n",
    "\n",
    "# Function to process all text files in a directory\n",
    "def process_directory(input_dir, output_dir, max_words_per_chunk=1024, overlap_words=200):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    for file_name in tqdm(os.listdir(input_dir)):\n",
    "        if file_name.endswith(\".txt\"):\n",
    "            file_path = os.path.join(input_dir, file_name)\n",
    "            file_base_name = os.path.splitext(file_name)[0]\n",
    "\n",
    "            # Read file content and shard it\n",
    "            text = read_file(file_path)\n",
    "            shards = shard_text_by_words(text, max_words_per_chunk, overlap_words)\n",
    "\n",
    "            # Save each shard\n",
    "            for index, shard in enumerate(shards):\n",
    "                save_shard(shard, output_dir, file_base_name, index)\n",
    "\n",
    "    print(\"All files processed.\")\n",
    "\n",
    "# Define input and output directories\n",
    "input_directory = '../../data/crawled/crawled_text_data'  # Directory containing text files\n",
    "output_directory = '../../data/crawled/crawled_text_data_test'  # Directory to save the shards\n",
    "\n",
    "# Process the directory and shard files\n",
    "process_directory(input_directory, output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/alan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "100%|██████████| 161/161 [00:03<00:00, 50.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files processed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Make sure NLTK's sentence and word tokenizers are downloaded\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Function to read the content of a text file\n",
    "def read_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "# Function to split text into sentence-based chunks with word-based overlap\n",
    "def shard_text_by_sentences(text, max_words_per_chunk=1024, overlap_words=200):\n",
    "    sentences = sent_tokenize(text)  # Tokenize text into sentences\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_word_count = 0\n",
    "\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        sentence_words = word_tokenize(sentence)\n",
    "        sentence_word_count = len(sentence_words)\n",
    "\n",
    "        # Check if adding this sentence will exceed the word limit for the chunk\n",
    "        if current_word_count + sentence_word_count > max_words_per_chunk:\n",
    "            # Finalize the current chunk\n",
    "            chunks.append(current_chunk)\n",
    "\n",
    "            # Reset current_chunk with overlap from the previous chunk and new sentence\n",
    "            overlap = current_chunk[-overlap_words:] if len(current_chunk) > overlap_words else current_chunk\n",
    "            current_chunk = overlap + sentence_words\n",
    "            current_word_count = len(current_chunk)\n",
    "\n",
    "        else:\n",
    "            current_chunk += sentence_words\n",
    "            current_word_count += sentence_word_count\n",
    "\n",
    "    # Add the last chunk if it's not empty\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Function to save each chunk into a separate text file\n",
    "def save_shard(chunk, output_dir, file_name, shard_index):\n",
    "    output_file = os.path.join(output_dir, f\"{file_name}-{shard_index}.txt\")\n",
    "    \n",
    "    # Join words in the chunk and save to file\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        file.write(' '.join(chunk))\n",
    "\n",
    "# Function to process all text files in a directory\n",
    "def process_directory(input_dir, output_dir, max_words_per_chunk=1024, overlap_words=200):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    for file_name in tqdm(os.listdir(input_dir)):\n",
    "        if file_name.endswith(\".txt\"):\n",
    "            file_path = os.path.join(input_dir, file_name)\n",
    "            file_base_name = os.path.splitext(file_name)[0]\n",
    "\n",
    "            # Read file content and shard it\n",
    "            text = read_file(file_path)\n",
    "            shards = shard_text_by_sentences(text, max_words_per_chunk, overlap_words)\n",
    "\n",
    "            # Save each shard\n",
    "            for index, shard in enumerate(shards):\n",
    "                save_shard(shard, output_dir, file_base_name, index)\n",
    "\n",
    "    print(\"All files processed.\")\n",
    "\n",
    "# Define input and output directories\n",
    "input_directory = '../../data/crawled/crawled_text_data'  # Directory containing text files\n",
    "output_directory = '../../data/crawled/crawled_text_data_50_sentence'  # Directory to save the shards\n",
    "\n",
    "# Process the directory and shard files\n",
    "process_directory(input_directory, output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/alan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "  0%|          | 0/688 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 688/688 [00:03<00:00, 223.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files processed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Make sure NLTK's word tokenizer is downloaded\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Function to read the content of a text file\n",
    "def read_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "# Function to split a large chunk into smaller chunks if it exceeds 1024 words\n",
    "def split_chunk_by_words(text, max_words_per_chunk=1024):\n",
    "    words = word_tokenize(text)  # Tokenize the chunk into words\n",
    "    chunks = []\n",
    "    \n",
    "    for i in range(0, len(words), max_words_per_chunk):\n",
    "        chunk = words[i:i + max_words_per_chunk]\n",
    "        chunks.append(chunk)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Function to save the new split chunks into separate text files\n",
    "def save_shard(chunk, output_dir, file_name, shard_index):\n",
    "    output_file = os.path.join(output_dir, f\"{file_name}-{shard_index}.txt\")\n",
    "    \n",
    "    # Join words in the chunk and save to file\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        file.write(' '.join(chunk))\n",
    "\n",
    "# Function to process the directory of chunks and further split chunks larger than 1024 words\n",
    "def process_chunk_directory(input_dir, output_dir, max_words_per_chunk=1024):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    for file_name in tqdm(os.listdir(input_dir)):\n",
    "        if file_name.endswith(\".txt\"):\n",
    "            file_path = os.path.join(input_dir, file_name)\n",
    "            file_base_name = os.path.splitext(file_name)[0]\n",
    "\n",
    "            # Read the chunk file content\n",
    "            text = read_file(file_path)\n",
    "            \n",
    "            # Check the word count of the chunk and split if necessary\n",
    "            chunks = split_chunk_by_words(text, max_words_per_chunk)\n",
    "\n",
    "            # Save the new smaller chunks\n",
    "            for index, chunk in enumerate(chunks):\n",
    "                save_shard(chunk, output_dir, file_base_name, index)\n",
    "\n",
    "    print(\"All files processed.\")\n",
    "\n",
    "# Define input and output directories\n",
    "input_directory = '../../data/crawled/crawled_text_data_sentence_50_5'  # Directory containing text files\n",
    "output_directory = '../../data/crawled/crawled_text_data_max_1024'  # Directory to save the shards\n",
    "\n",
    "# Process the directory and split chunks if needed\n",
    "process_chunk_directory(input_directory, output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
