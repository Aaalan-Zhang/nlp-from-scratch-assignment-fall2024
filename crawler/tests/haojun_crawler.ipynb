{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import re\n",
    "import time\n",
    "# from datetime import datetime\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "# import pycurl\n",
    "import codecs\n",
    "import base64\n",
    "import pandas as pd\n",
    "\n",
    "colName = ['operationStatus', 'registrationCode', 'name', 'type', 'administrator', 'certTime', 'registrationBereau', 'representative', 'date', 'funding', 'scope', 'headquater']\n",
    "\n",
    "dataCollection = {}\n",
    "\n",
    "keyword = '肿瘤'\n",
    "for i in colName:\n",
    "    dataCollection[i] = []\n",
    "    \n",
    "for file in os.listdir(f'/Users/alan/Desktop/keywords/keyword={keyword}'):\n",
    "    if 'html' in file:\n",
    "        f = codecs.open(f'/Users/alan/Desktop/keywords/keyword={keyword}/{file}', 'r')\n",
    "        test = f.read()\n",
    "        result = re.findall('<span data-v-6e51d4b3=\"\">统一社会信用代码:\\n                        <span data-v-6e51d4b3=\"\">\\n                    <span>.*</span>', test)\n",
    "        urlList = []\n",
    "        for i in range(0, len(result)):\n",
    "            result[i] = '{\"id\":\"' + result[i][-25:-7] + '\"}'\n",
    "            base64Encoded = base64.b64encode(bytes(result[i], 'utf-8'), altchars=None).decode(\"utf-8\")\n",
    "            urlList.append(f'https://xxgs.chinanpo.mca.gov.cn/gsxt/newDetails?b={base64Encoded}')\n",
    "\n",
    "        for i in tqdm(range(0, len(urlList))):\n",
    "            driverPath = '/Users/alan/Downloads/chromedriver'\n",
    "            driver = webdriver.Chrome(driverPath) \n",
    "            driver.get(urlList[i])\n",
    "            time.sleep(1)\n",
    "            response = driver.page_source\n",
    "\n",
    "            try:\n",
    "                operationStatus = re.findall(r'[\\u4e00-\\u9fff]+', re.findall('class=\"ant-tag ant-tag-has-color\" style=\"background-color:.*</div></div><div data-v-283f9344=\"\" class=\"tag_center\">', response)[0])\n",
    "            except:\n",
    "                operationStatus = None\n",
    "            try:\n",
    "                name = re.findall('<span data-v-d14c252c=\"\" class=\"data_break_line\">\\n.*\\n', re.findall('社会组织名称\\n.*社会组织类型\\n', response, re.DOTALL)[0], re.DOTALL)[0].split('\\n')[1].strip()\n",
    "            except:\n",
    "                name = None\n",
    "            try:\n",
    "                type = re.findall('<span data-v-d14c252c=\"\" class=\"data_break_line\">\\n.*\\n', re.findall('社会组织类型\\n.*业务主管单位\\n ', response, re.DOTALL)[0], re.DOTALL)[0].split('\\n')[1].strip()\n",
    "            except:\n",
    "                type = None\n",
    "            try:\n",
    "                administrator = re.findall('<span data-v-d14c252c=\"\" class=\"data_break_line\">\\n.*\\n', re.findall('业务主管单位\\n.*证书有效期\\n', response, re.DOTALL)[0], re.DOTALL)[0].split('\\n')[1].strip()\n",
    "            except:\n",
    "                administrator = None\n",
    "            try:\n",
    "                certTime = re.findall('<span data-v-d14c252c=\"\" class=\"data_break_line\">\\n.*\\n', re.findall('证书有效期\\n.*登记管理机关\\n', response, re.DOTALL)[0], re.DOTALL)[0].split('\\n')[1].strip()\n",
    "            except:\n",
    "                certTime = None\n",
    "            try:\n",
    "                registrationBereau = re.findall('<span data-v-d14c252c=\"\" class=\"data_break_line\">\\n.*\\n', re.findall('登记管理机关\\n.*法定代表人\\n', response, re.DOTALL)[0], re.DOTALL)[0].split('\\n')[1].strip()\n",
    "            except:\n",
    "                registrationBereau = None\n",
    "            try:\n",
    "                representative = re.findall('<span data-v-d14c252c=\"\" class=\"data_break_line\">\\n.*\\n', re.findall('法定代表人\\n.*成立登记日期\\n', response, re.DOTALL)[0], re.DOTALL)[0].split('\\n')[1].strip()\n",
    "            except:\n",
    "                representative = None\n",
    "            try:\n",
    "                date = re.findall('<span data-v-d14c252c=\"\" class=\"data_break_line\">\\n.*\\n', re.findall('成立登记日期\\n.*注册资金', response, re.DOTALL)[0], re.DOTALL)[0].split('\\n')[1].strip()\n",
    "            except:\n",
    "                date = None\n",
    "            try:\n",
    "                funding = re.findall(r'[\\d]+', re.findall('<span data-v-d14c252c=\"\" class=\"data_break_line\">\\n.*\\n', re.findall('注册资金.*业务范围\\n', response, re.DOTALL)[0], re.DOTALL)[0].split('\\n')[1].strip())[0] + \\\n",
    "                            re.findall(r'[\\u4e00-\\u9fff]+', re.findall('<span data-v-d14c252c=\"\" class=\"data_break_line\">\\n.*\\n', re.findall('注册资金.*业务范围\\n', response, re.DOTALL)[0], re.DOTALL)[0].split('\\n')[1].strip())[0]\n",
    "            except:\n",
    "                funding = None\n",
    "            try:\n",
    "                scope = re.findall(r'[\\u4e00-\\u9fff]+', re.findall('span data-v-d14c252c=\"\" class=\"text long_text2\"><span data-v-d14c252c=\"\" class=\"aaae0122Txt\">\\n.*\\n', re.findall('业务范围\\n.*住所\\n', response, re.DOTALL)[0], re.DOTALL)[0])[:-1]\n",
    "            except:\n",
    "                scope = None\n",
    "            try:\n",
    "                headquarter = re.findall('span data-v-d14c252c=\"\" class=\"text long_text2\"><span data-v-d14c252c=\"\" class=\"aaae0122Txt\">\\n.*\\n', re.findall('住所\\n.*点击或者下拉加载更多', response, re.DOTALL)[0], re.DOTALL)[0].split('\\n')[-2].strip()\n",
    "            except:\n",
    "                headquarter = None\n",
    "            this_collection = {\n",
    "                                'operationStatus': operationStatus,\n",
    "                                'registrationCode': result[i][7:-2],\n",
    "                                'name': name,\n",
    "                                'type': type,\n",
    "                                'administrator': administrator,\n",
    "                                'certTime': certTime,\n",
    "                                'registrationBereau': registrationBereau,\n",
    "                                'representative': representative,\n",
    "                                'date': date,\n",
    "                                'funding': funding,\n",
    "                                'scope': scope,\n",
    "                                'headquater': headquarter\n",
    "                            }\n",
    "            for i in colName:\n",
    "                dataCollection[i].append(this_collection[i])\n",
    "\n",
    "            driver.close()\n",
    "\n",
    "df = pd.DataFrame(data = dataCollection)\n",
    "df.to_csv(f'/Users/alan/Desktop/data_{keyword}.csv', index = None, encoding = 'utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from requests.exceptions import ConnectionError, Timeout, RequestException\n",
    "from bs4 import BeautifulSoup\n",
    "import warnings\n",
    "from urllib3.exceptions import InsecureRequestWarning\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Suppress InsecureRequestWarnings\n",
    "warnings.filterwarnings('ignore', category=InsecureRequestWarning)\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = 'data_source_1011_161_entries.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Extract non-empty URLs from the 'Source URL' column\n",
    "urls = data['Source URL'].dropna().unique()\n",
    "\n",
    "# Define a list of user-agents to simulate different browsers\n",
    "user_agents = [\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3',\n",
    "    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',\n",
    "    'Mozilla/5.0 (iPhone; CPU iPhone OS 11_0 like Mac OS X) AppleWebKit/604.1.38 (KHTML, like Gecko) Version/11.0 Mobile/15A372 Safari/604.1'\n",
    "]\n",
    "\n",
    "# Define headers for requests\n",
    "headers = {'User-Agent': random.choice(user_agents)}\n",
    "\n",
    "# Create a session to reuse connections\n",
    "session = requests.Session()\n",
    "\n",
    "# Function to scrape a web page and extract information\n",
    "def scrape_url(url, retries=3, delay=5, timeout=10):\n",
    "    attempt = 0\n",
    "    while attempt < retries:\n",
    "        try:\n",
    "            response = session.get(url, headers=headers, timeout=timeout, verify=False)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.content, 'lxml')\n",
    "\n",
    "            # Extract title\n",
    "            title = soup.title.string if soup.title else \"No title\"\n",
    "\n",
    "            # Extract headings\n",
    "            headings = [h.get_text(strip=True) for h in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])]\n",
    "\n",
    "            # Extract paragraphs\n",
    "            paragraphs = [p.get_text(strip=True) for p in soup.find_all('p')]\n",
    "\n",
    "            # Extract links\n",
    "            links = [a['href'] for a in soup.find_all('a', href=True)]\n",
    "\n",
    "            # Extract meta description\n",
    "            meta_description = soup.find('meta', attrs={'name': 'description'})\n",
    "            description = meta_description['content'] if meta_description else \"No meta description\"\n",
    "\n",
    "            # Extract images\n",
    "            images = [img['src'] for img in soup.find_all('img', src=True)]\n",
    "\n",
    "            return {\n",
    "                \"Title\": title,\n",
    "                \"Headings\": headings,\n",
    "                \"Paragraphs\": paragraphs,\n",
    "                \"Links\": links,\n",
    "                \"Description\": description,\n",
    "                \"Images\": images\n",
    "            }\n",
    "\n",
    "        except (ConnectionError, Timeout, RequestException) as e:\n",
    "            attempt += 1\n",
    "            if attempt < retries:\n",
    "                print(f\"Retrying {url}, attempt {attempt}/{retries} after {delay} seconds.\")\n",
    "                time.sleep(delay)\n",
    "            else:\n",
    "                return f\"Failed to retrieve {url}: {e}\"\n",
    "        except Exception as e:\n",
    "            return f\"Parsing failed for {url}: {e}\"\n",
    "\n",
    "# Scrape each URL and store the results\n",
    "scraped_data = []\n",
    "for url in urls:\n",
    "    info = scrape_url(url)\n",
    "    scraped_data.append((url, info))\n",
    "\n",
    "# Convert scraped data to a DataFrame for display\n",
    "scraped_df = pd.DataFrame(scraped_data, columns=['URL', 'Website Information'])\n",
    "\n",
    "# Display the scraped data\n",
    "print(scraped_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
