{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sublink processing hasn't been added (but is necessary)\n",
    "selenium parsing may be useful but after some tests, it turns out to be not very useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def fetch_page_text(url):\n",
    "    try:\n",
    "        # Send a GET request to the URL\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an error for bad responses\n",
    "        # Parse the page content\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        # Extract and return the text from the page\n",
    "        return soup.get_text(separator='\\n', strip=True)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def crawl_urls(url_list):\n",
    "    results = {}\n",
    "    for url in url_list:\n",
    "        print(f\"Fetching: {url}\")\n",
    "        text = fetch_page_text(url)\n",
    "        if text:\n",
    "            results[url] = text\n",
    "    return results\n",
    "\n",
    "def save_crawled_data(crawled_data, urls):\n",
    "    # Iterate over each URL and its corresponding text\n",
    "    for index, url in enumerate(urls):\n",
    "        # Fetch the text corresponding to the URL from the crawled_data dictionary\n",
    "        text = crawled_data.get(url, \"\")\n",
    "\n",
    "        # Remove all newline characters from the text\n",
    "        cleaned_text = text.replace('\\n', ' ')\n",
    "\n",
    "        # Define the file name with the index from the URLs list\n",
    "        output_file = f\"/Users/alan/11711/nlp-from-scratch-assignment/data/1010_160_entries/crawled/crawled_text_data/{index}.txt\"\n",
    "\n",
    "        # Save the cleaned text to the file\n",
    "        with open(output_file, 'w', encoding='utf-8') as file:\n",
    "            file.write(cleaned_text)\n",
    "\n",
    "        print(f\"Saved content from URL {url} to {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = '/Users/alan/11711/nlp-from-scratch-assignment/data/1010_160_entries/raw/raw_csv_data/data_source.csv'\n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    # Extract non-empty URLs from the 'Source URL' column\n",
    "    urls = data[data['Select'] == 'Webpage']['Source URL'].dropna().unique()\n",
    "    # Start crawling the URLs\n",
    "    crawled_data = crawl_urls(urls)\n",
    "    \n",
    "    # Print or process the results\n",
    "    save_crawled_data(crawled_data, urls)\n",
    "    print(\"Crawling complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the code for Selenium web driver crawling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Function to fetch the page content using Selenium\n",
    "def fetch_page_text_selenium(url):\n",
    "    try:\n",
    "        driver = webdriver.Chrome(driverPath) \n",
    "        # Initialize Chrome WebDriver\n",
    "        driver.get(url)\n",
    "        \n",
    "        # Wait for the page to load (adjust time if needed)\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Get page source and close the browser\n",
    "        page_source = driver.page_source\n",
    "        driver.quit()\n",
    "\n",
    "        # Parse the page content using BeautifulSoup\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "        # Extract and clean the text from the page\n",
    "        page_text = soup.get_text(separator='\\n', strip=True)\n",
    "        return page_text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to read URLs from CSV and crawl each one\n",
    "def crawl_urls_from_csv(csv_file_path, url_column_name):\n",
    "    with open(csv_file_path, newline='', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for index, row in enumerate(reader):\n",
    "            url = row[url_column_name]\n",
    "            print(f\"Fetching: {url}\")\n",
    "            text = fetch_page_text_selenium(url)\n",
    "            if text:\n",
    "                # Save the crawled text to a file with the index as the filename\n",
    "                output_file = f\"/Users/alan/11711/nlp-from-scratch-assignment/data/1010_160_entries/crawled/crawled_text_data/{index}.txt\"\n",
    "                with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                    f.write(text)\n",
    "                print(f\"Saved content to {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    csv_file_path = '/Users/alan/11711/nlp-from-scratch-assignment/data/1010_160_entries/raw/raw_csv_data/data_source.csv'\n",
    "    url_column_name = 'Source URL'\n",
    "    driverPath = '/Users/alan/Downloads/chromedriver-mac-arm64/chromedriver'\n",
    "\n",
    "\n",
    "    # Start crawling the URLs from the CSV\n",
    "    crawl_urls_from_csv(csv_file_path, url_column_name)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
