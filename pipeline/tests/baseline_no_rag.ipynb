{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QA Baseline without RAG\n",
    "\n",
    "This notebook is a baseline for the QA task without the RAG model. For a fair comparison, we choose the same backbone model as the one in the RAG pipeline: the `meta/llama3.1-8b-Instruct` model. We also adopt the same data type (fp16) and the same config for setting up the tokenizer. We use the same prompt format as the one in the RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4471b193e55249f8889a8f1b8e464f2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "generation_pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer, \n",
    "    torch_dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: load qa annotation test set\n",
    "import pandas as pd\n",
    "qa_df = pd.read_csv(\"../../data/annotated/generated_qa_pairs_3000_test20.csv\")\n",
    "\n",
    "doc_ids = qa_df[\"Doc_id\"].tolist()\n",
    "questions = qa_df[\"Question\"].tolist()\n",
    "answers = qa_df[\"Answer\"].tolist()\n",
    "\n",
    "# random sample 10 qa pairs\n",
    "import random\n",
    "sample_size = 10\n",
    "random.seed(747)\n",
    "sample_indices = random.sample(range(len(questions)), sample_size)\n",
    "sample_doc_ids = [doc_ids[i] for i in sample_indices]\n",
    "sample_questions = [questions[i] for i in sample_indices]\n",
    "sample_answers = [answers[i] for i in sample_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "You are an expert assistant answering factual questions about various aspects of Pittsburgh or Carnegie Mellon University (CMU), including history, policy, culture, events, and more. \n",
    "If you do not know the answer, just say \"I don't know.\"\n",
    "\n",
    "Important Instructions:\n",
    "- Answer concisely without repeating the question.\n",
    "- Do **not** use complete sentences. Provide only the word, name, date, or phrase that directly answers the question. For example, given the question \"When was Carnegie Mellon University founded?\", you should only answer \"1900\".\n",
    "\n",
    "Examples:\n",
    "Question: Who is Pittsburgh named after? \n",
    "Answer: William Pitt\n",
    "Question: What famous machine learning venue had its first conference in Pittsburgh in 1980? \n",
    "Answer: ICML\n",
    "Question: What musical artist is performing at PPG Arena on October 13? \n",
    "Answer: Billie Eilish\n",
    "\n",
    "Question: {question} \\n\\n\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    }
   ],
   "source": [
    "# use the template the generate the answers\n",
    "generated_answers = []\n",
    "for question in sample_questions:\n",
    "    full_prompt = template.format(question=question)\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": full_prompt},\n",
    "        ]\n",
    "    output = generation_pipe(messages, max_new_tokens=50)\n",
    "    generated_answers.append(output[0][\"generated_text\"][1]['content'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Richard Thornburgh', \"I don't know\", 'Three Rivers Princess', \"I don't know\", 'Homewood North', 'Michael Chabon', 'Dava Newman', \"I don't know\", 'Overture', 'Heinz Hall']\n",
      "['Dick Thornburgh', 'Chicago Cubs', \"Captain's Dinner Cruise & Tour\", 'Julia Leyzarovich', '125 acres', 'John Edgar Wideman', 'Jay Apt', 'E. W. Marland', 'Kimberly Akimbo', 'Lang Lang']\n"
     ]
    }
   ],
   "source": [
    "print(generated_answers)\n",
    "print(sample_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write all columns to a csv file\n",
    "results_df = pd.DataFrame({\n",
    "        \"Ref Doc id\": sample_doc_ids,\n",
    "        \"Question\": sample_questions,\n",
    "        \"Ref Answer\": sample_answers,\n",
    "        \"Generated Answer\": generated_answers,\n",
    "    })\n",
    "\n",
    "# save the results to a csv file\n",
    "results_df.to_csv(\"../../output/baseline_results.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
