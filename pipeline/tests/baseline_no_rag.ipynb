{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QA Baseline without RAG\n",
    "\n",
    "This notebook is a baseline for the QA task without the RAG model. For a fair comparison, we choose the same backbone model as the one in the RAG pipeline: the `meta/llama3.1-8b-Instruct` model. We also adopt the same data type (fp16) and the same config for setting up the tokenizer. We use the same prompt format as the one in the RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "867940a0103d44219ffa0c919eb1f83a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "generation_pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer, \n",
    "    torch_dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: load qa annotation test set\n",
    "import pandas as pd\n",
    "qa_df = pd.read_csv(\"../../data/annotated/generated_qa_pairs_3000_test20.csv\")\n",
    "\n",
    "questions = qa_df[\"Question\"].tolist()\n",
    "answers = qa_df[\"Answer\"].tolist()\n",
    "\n",
    "# random sample 10 qa pairs\n",
    "import random\n",
    "sample_size = 10\n",
    "random.seed(221)\n",
    "sample_indices = random.sample(range(len(questions)), sample_size)\n",
    "sample_questions = [questions[i] for i in sample_indices]\n",
    "sample_answers = [answers[i] for i in sample_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "You are an expert assistant answering factual questions about various aspects of Pittsburgh or Carnegie Mellon University (CMU), including history, policy, culture, events, and more. \n",
    "If you do not know the answer, just say \"I don't know.\"\n",
    "\n",
    "Important Instructions:\n",
    "- Answer concisely without repeating the question.\n",
    "- Do **not** use complete sentences. Provide only the word, name, date, or phrase that directly answers the question. For example, given the question \"When was Carnegie Mellon University founded?\", you should only answer \"1900\".\n",
    "\n",
    "Examples:\n",
    "Question: Who is Pittsburgh named after? \n",
    "Answer: William Pitt\n",
    "Question: What famous machine learning venue had its first conference in Pittsburgh in 1980? \n",
    "Answer: ICML\n",
    "Question: What musical artist is performing at PPG Arena on October 13? \n",
    "Answer: Billie Eilish\n",
    "\n",
    "Question: {question} \\n\\n\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the template the generate the answers\n",
    "generated_answers = []\n",
    "for question in sample_questions:\n",
    "    full_prompt = template.format(question=question)\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": full_prompt},\n",
    "        ]\n",
    "    output = generation_pipe(messages, max_new_tokens=50)\n",
    "    generated_answers.append(output[0][\"generated_text\"][1]['content'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Opera Guild', 'WYO', 'Family Friendly Opera', '1946', 'Robin Williams', \"I don't know\", 'David L. Lawrence Convention Center', \"I don't know.\", 'Pennsylvania', '1, 138,000']\n",
      "['Monteverdi Society', 'Pittsburgh Jazz, Blues, and Bluegrass', 'La Traviata', '2024', 'Adrian Cronauer', 'New Jersey Devils', \"The city's convention center\", '4.5 seconds', 'Pennsylvania', '1,846,000']\n"
     ]
    }
   ],
   "source": [
    "print(generated_answers)\n",
    "print(sample_answers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
